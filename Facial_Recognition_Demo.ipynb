{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2322f5",
   "metadata": {},
   "source": [
    "### Current Progress/Ideas:\n",
    "\n",
    "Using deepface to analyze images is too slow, e.g. takes 20-30 seconds per image. Likely unable to use for short demo unless significant efficiency improvements can be made.\n",
    "\n",
    "Instead, can possibly use pretrained model on data. Split into training/test and use model to make predictions. Currently using pixel values as feature data for model but can use actual images instead, either in the wild or cropped.\n",
    "\n",
    "https://www.kaggle.com/datasets/abhikjha/utk-face-cropped\n",
    "\n",
    "https://susanqq.github.io/UTKFace/\n",
    "\n",
    "possible race trained model?\n",
    "https://colab.research.google.com/drive/1MMTe7XPvvVuweAxBcc_erHNV1haWoH4S?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3feaf08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install deepface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d35d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 06:16:36.081457: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-05-25 06:16:36.132602: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from PIL import Image\n",
    "#from tensorflow.keras.applications import ResNet50\n",
    "#from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "#from tensorflow.keras.preprocessing import image\n",
    "# Face recognition and facial attribute analysis framework\n",
    "#from deepface import DeepFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "930c94a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet50 model trained on ImageNet\n",
    "#model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1115dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe containing a list of 5 attributes for each image.\n",
    "# In particular, race is labelled with a number from 0-4, \n",
    "# denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern)\n",
    "face_attr = pd.read_csv('age_gender.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee0e713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>gender</th>\n",
       "      <th>img_name</th>\n",
       "      <th>pixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219203650636.jpg.chip.jpg</td>\n",
       "      <td>129 128 128 126 127 130 133 135 139 142 145 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222752047.jpg.chip.jpg</td>\n",
       "      <td>164 74 111 168 169 171 175 182 184 188 193 199...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161219222832191.jpg.chip.jpg</td>\n",
       "      <td>67 70 71 70 69 67 70 79 90 103 116 132 145 155...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144911423.jpg.chip.jpg</td>\n",
       "      <td>193 197 198 200 199 200 202 203 204 205 208 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>20161220144914327.jpg.chip.jpg</td>\n",
       "      <td>202 205 209 210 209 209 210 211 212 214 218 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23700</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170120221920654.jpg.chip.jpg</td>\n",
       "      <td>127 100 94 81 77 77 74 99 102 98 128 145 160 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23701</th>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20170120134639935.jpg.chip.jpg</td>\n",
       "      <td>23 28 32 35 42 47 68 85 98 103 113 117 130 129...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23702</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20170110182418864.jpg.chip.jpg</td>\n",
       "      <td>59 50 37 40 34 19 30 101 156 170 177 184 187 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23703</th>\n",
       "      <td>99</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20170117195405372.jpg.chip.jpg</td>\n",
       "      <td>45 108 120 156 206 197 140 180 191 199 204 207...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23704</th>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20170110182052119.jpg.chip.jpg</td>\n",
       "      <td>156 161 160 165 170 173 166 177 183 191 187 18...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23705 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  ethnicity  gender                        img_name  \\\n",
       "0        1          2       0  20161219203650636.jpg.chip.jpg   \n",
       "1        1          2       0  20161219222752047.jpg.chip.jpg   \n",
       "2        1          2       0  20161219222832191.jpg.chip.jpg   \n",
       "3        1          2       0  20161220144911423.jpg.chip.jpg   \n",
       "4        1          2       0  20161220144914327.jpg.chip.jpg   \n",
       "...    ...        ...     ...                             ...   \n",
       "23700   99          0       1  20170120221920654.jpg.chip.jpg   \n",
       "23701   99          1       1  20170120134639935.jpg.chip.jpg   \n",
       "23702   99          2       1  20170110182418864.jpg.chip.jpg   \n",
       "23703   99          2       1  20170117195405372.jpg.chip.jpg   \n",
       "23704   99          0       1  20170110182052119.jpg.chip.jpg   \n",
       "\n",
       "                                                  pixels  \n",
       "0      129 128 128 126 127 130 133 135 139 142 145 14...  \n",
       "1      164 74 111 168 169 171 175 182 184 188 193 199...  \n",
       "2      67 70 71 70 69 67 70 79 90 103 116 132 145 155...  \n",
       "3      193 197 198 200 199 200 202 203 204 205 208 21...  \n",
       "4      202 205 209 210 209 209 210 211 212 214 218 21...  \n",
       "...                                                  ...  \n",
       "23700  127 100 94 81 77 77 74 99 102 98 128 145 160 1...  \n",
       "23701  23 28 32 35 42 47 68 85 98 103 113 117 130 129...  \n",
       "23702  59 50 37 40 34 19 30 101 156 170 177 184 187 1...  \n",
       "23703  45 108 120 156 206 197 140 180 191 199 204 207...  \n",
       "23704  156 161 160 165 170 173 166 177 183 191 187 18...  \n",
       "\n",
       "[23705 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb186a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 48 x 48 pixel images\n",
    "len(face_attr['pixels'][0].split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5e48cc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ethnicity\n",
       "0    10078\n",
       "1     4526\n",
       "3     3975\n",
       "2     3434\n",
       "4     1692\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_attr['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e6c7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No matching data found for 20170116174525125.jpg.chip.jpg\n",
      "No matching data found for 20170109150557335.jpg.chip.jpg\n",
      "No matching data found for utkcropped\n",
      "No matching data found for 20170109150557335.jpg.chip.jpg\n",
      "No matching data found for 20170109142408075.jpg.chip.jpg\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir('utkcropped')\n",
    "directory = 'utkcropped'\n",
    "images = []\n",
    "labels = []\n",
    "for filename in files:\n",
    "    new_filename = filename.split('_')[-1]\n",
    "    img_path = os.path.join(directory, filename)\n",
    "    matching_row = face_attr[face_attr['img_name'] == new_filename]\n",
    "    \n",
    "    try:\n",
    "        if not matching_row.empty:\n",
    "            ethnicity = matching_row.iloc[0]['ethnicity']\n",
    "            img = Image.open(img_path)  # Open the image\n",
    "            img_array = np.array(img)  # Convert the image to a NumPy array\n",
    "            # Append the new name, image array, and additional data to the list\n",
    "            images.append(img_array)\n",
    "            labels.append(ethnicity)\n",
    "        else:\n",
    "            print(f\"No matching data found for {new_filename}\")\n",
    "    except Exception as e:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c88e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "y = np.array(labels)\n",
    "y = to_categorical(y, len(np.unique(face_attr['ethnicity'])))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65674046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 06:19:06.521341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9622 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:61:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-25 06:19:11.432204: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-05-25 06:19:12.261952: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f67e13d8990 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-05-25 06:19:12.262069: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 2080 Ti, Compute Capability 7.5\n",
      "2024-05-25 06:19:12.279788: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-05-25 06:19:12.456461: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "475/475 [==============================] - 16s 24ms/step - loss: 5.5242 - accuracy: 0.5142 - val_loss: 1.0125 - val_accuracy: 0.6325\n",
      "Epoch 2/10\n",
      "475/475 [==============================] - 11s 23ms/step - loss: 0.9626 - accuracy: 0.6539 - val_loss: 0.9088 - val_accuracy: 0.6805\n",
      "Epoch 3/10\n",
      "475/475 [==============================] - 10s 21ms/step - loss: 1.0251 - accuracy: 0.6406 - val_loss: 0.9855 - val_accuracy: 0.6517\n",
      "Epoch 4/10\n",
      "475/475 [==============================] - 10s 22ms/step - loss: 0.8140 - accuracy: 0.7127 - val_loss: 0.8616 - val_accuracy: 0.6994\n",
      "Epoch 5/10\n",
      "475/475 [==============================] - 11s 22ms/step - loss: 0.7133 - accuracy: 0.7427 - val_loss: 0.9814 - val_accuracy: 0.6852\n",
      "Epoch 6/10\n",
      "475/475 [==============================] - 11s 22ms/step - loss: 0.6292 - accuracy: 0.7789 - val_loss: 0.9216 - val_accuracy: 0.6989\n",
      "Epoch 7/10\n",
      "475/475 [==============================] - 11s 22ms/step - loss: 0.5282 - accuracy: 0.8123 - val_loss: 1.0712 - val_accuracy: 0.6976\n",
      "Epoch 8/10\n",
      "475/475 [==============================] - 11s 22ms/step - loss: 0.4540 - accuracy: 0.8391 - val_loss: 1.0595 - val_accuracy: 0.6755\n",
      "Epoch 9/10\n",
      "475/475 [==============================] - 11s 22ms/step - loss: 0.4014 - accuracy: 0.8624 - val_loss: 1.2356 - val_accuracy: 0.6781\n",
      "Epoch 10/10\n",
      "475/475 [==============================] - 10s 22ms/step - loss: 0.3257 - accuracy: 0.8856 - val_loss: 1.2971 - val_accuracy: 0.6923\n",
      "149/149 [==============================] - 1s 9ms/step - loss: 1.3718 - accuracy: 0.6849\n",
      "Test accuracy: 0.6848766207695007\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(y[0])\n",
    "\n",
    "# Example CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(200,200,3)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate on test set if available\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524d3d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149/149 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Convert predictions from one-hot encoded back to categorical labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "true_labels = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3755fca5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 2340\n",
      "1: 911\n",
      "2: 662\n",
      "3: 707\n",
      "4: 121\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(predicted_labels, return_counts=True)\n",
    "\n",
    "# Print the predicted label counts\n",
    "for element, count in zip(unique_elements, counts):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a737ec94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 1963\n",
      "1: 917\n",
      "2: 717\n",
      "3: 819\n",
      "4: 325\n"
     ]
    }
   ],
   "source": [
    "unique_elements, counts = np.unique(true_labels, return_counts=True)\n",
    "\n",
    "# Print the true label counts\n",
    "for element, count in zip(unique_elements, counts):\n",
    "    print(f\"{element}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc214ab",
   "metadata": {},
   "source": [
    "Race is labelled with a number from 0-4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern)\n",
    "\n",
    "Here, the model is doing a fairly decent job predicting the race/ethnicity labels in the data with ~68% accuracy. However, if we look at the predicted labels and compare them to the true labels, we can see that the model is actually overpredicting one specific race (white) by a significant margin (almost 20% higher than it occurs in reality).\n",
    "\n",
    "This is sort of what I think would be good to demonstrate in the demo? Essentially, if we train our model on some disproportionate/biased data (original dataset is primarily composed of white people), we can get biased model outputs."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
